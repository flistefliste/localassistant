# Localassistant.pangloss.io

## Install Ollama locally
```
# macOS
brew install ollama         
# or for Ubuntu
curl -fsSL https://ollama.com/install.sh | sh
# Windows
https://ollama.com/download/windows
```

## Config and run Ollama locally
```
$ ollama serve
$ ollama pull llama3.1
$ ollama run llama3.1
```

## Start the backend
```
$ npm run start
```

## Start the frontend
```
$ cd front && npm run dev
# or
$ cd front && npm run start
```

## Browse app locally
```
http://localhost:5173/
```